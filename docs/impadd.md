# Implementation Report: Cooperative Jamming Decision-Making (MA-CJD)

## 1. Introduction

This report details the implementation of a Multi-Agent Reinforcement Learning (MARL) based Cooperative Jamming Decision-Making method (MA-CJD) designed for complex electromagnetic game scenarios involving multiple active jammers and radar systems[cite: 2]. The MA-CJD algorithm aims to achieve efficient target allocation, jamming mode selection, and power control[cite: 3]. It models the cooperative jamming process as a Markov game and utilizes the QMix MARL algorithm combined with a Multi-Pass Deep Q-Network (MP-DQN) structure to handle the parameterized action space[cite: 5, 6]. The objective is to minimize the detection time of defense units by radar while conserving jamming resources[cite: 7].

## 2. Implementation Details

### 2.1 System Architecture:
* A **Client-Server** architecture is used for the reinforcement learning training system.
* **Server**: Implemented in C++, handling the simulation environment, including jammer models, radar models, and entity interaction logic. It provides APIs for environment creation, reset, and step execution. Multiple independent environment instances run in parallel on the server for efficient sample collection.
* **Client**: Implemented in Python 3.12, managing the experience replay buffer, agent training (MA-CJD algorithm), and decision-making processes. Agent-environment interactions use the server's APIs[cite: 224].

### 2.2 Core Technologies:
* **Programming Languages**: C++ (Server), Python 3.12 (Client)[cite: 222, 223].
* **Machine Learning Library**: PyTorch 2.4 with CUDA 12.4 environment[cite: 226].
* **Algorithm**: MA-CJD, integrating QMix [cite: 155, 161] and MP-DQN [cite: 155, 177] with a Double DQN mechanism[cite: 156, 197].

### 2.3 Network Structures (MA-CJD):
* **Agent Network (MP-DQN based)**:
    * **Actor Network**: Generates the continuous jamming power level ($P^i$) for each discrete action ($T^i$). Implemented as a three-layer fully connected network (Input: 128 neurons, Hidden: 128 neurons, both ReLU activated)[cite: 183, 230]. The output uses a Sigmoid function to map power levels to the range [0, 1][cite: 231].
    * **Multi-Pass Q Network (MQN)**: Computes action value estimates ($Q^a$) taking state and the Actor-generated power level as input. Input layer has 128 neurons (Linear + ReLU)[cite: 227]. Hidden layer uses 128-dimensional GRU units[cite: 185, 228]. Output layer is a linear layer with 9 neurons (matching discrete actions)[cite: 228].
* **Mixing Network (QMix)**: Aggregates individual agent Q-values into a global Q-value ($Q_{total}$). Implemented as a two-layer hyper-network using ELU activation functions[cite: 232]. The first layer has 64 dimensions, output layer has 1 neuron[cite: 233]. Weights and biases are generated by linear layers (128 dimensions), with weights constrained to absolute values to ensure monotonicity (Individual-Global-Max condition)[cite: 234].

## 3. Configuration Values & Hyperparameters

### 3.1 Training Configuration:
* **Optimizer**: Adam optimizer for all networks[cite: 241].
* **Learning Rates**:
    * Q Network & Mixing Network ($\alpha$): 0.005[cite: 240].
    * Actor Network ($\beta$): 0.003[cite: 240].
* **Exploration Strategy**: $\epsilon$-greedy[cite: 186, 235].
    * Initial $\epsilon$: 0.95[cite: 235].
    * Final $\epsilon$: 0.05, reached after 100,000 training steps via linear decay[cite: 236].
    * $\epsilon$ fixed at 0.05 after 100,000 steps[cite: 237].
* **Experience Replay**:
    * Interaction per cycle: Agents interact with the environment for 16 episodes[cite: 242].
    * Training iterations per cycle: 4 iterations[cite: 243].
    * Batch size (B): 32 episodes sampled per training iteration[cite: 219, 243].
* **Target Network Update**: Double DQN mechanism used for calculating TD targets[cite: 197, 198, 207].
* **Training Duration**: 100,000 steps[cite: 244].
* **Hardware**: Intel Core i9-13900 CPU, Nvidia GeForce 4090 GPU[cite: 244].
* **Evaluation Frequency**: Every 100 training steps[cite: 245].
* **Evaluation Episodes**: 32 episodes per evaluation, using greedy action selection (highest value)[cite: 245, 247].

### 3.2 Reward Function Components:
* **Tracking Penalty ($r_d$)**: Applied when radar locks onto a defense unit. Value depends on radar threat level (manually defined hyperparameter, range -1.2 to -0.8 in experiments, specific values per radar type in Table 1 range from -0.9 to -1.1)[cite: 128, 129, 143, 203].
* **Resource Consumption Penalty ($r_p$)**: Linearly related to jamming power ($P^i$). Range: -0.1 (min power) to -0.01 (max power)[cite: 131, 133, 144].
* **Jamming Success Probability Reward ($r_j$)**: Calculated based on the probability of successful deception or suppression. Range: [0, 1][cite: 134, 135, 145]. Computed differently for deception (Eq. 13) [cite: 139, 140] and suppression (Eq. 14)[cite: 142].

### 3.3 Radar Model Parameters (Example):
* Refer to Table 1 in the original document for specific parameters ($P_t, D_m, D_s, r_d$) for the 4 radar types used in the simulation[cite: 203, 211]. *(Note: This section previously referenced Table 1 directly)*

### 3.4 Jamming Model Parameters:
* Jammer Transmission Power ($P_j$): Controlled via the continuous action $P^i \in [0, 1]$. Actual power calculated as $P_j = P^i \cdot (P_{j,max} - P_{j,min}) + P_{j,min}$[cite: 124, 126]. Specific $P_{j,max}$ and $P_{j,min}$ values are not explicitly stated but implied by the normalization.
* False Target Identification Probability ($pr_{ij}$): Calculated using Eq. (7), dependent on JNR[cite: 102, 106, 108]. Parameters $w$ and $JNR_{0.5}$ are used but specific values are not provided in the text[cite: 106].

## 4. Results and Analysis Summary

* **Effectiveness**: The MA-CJD algorithm significantly reduced the total radar lock time on defense units by over 60% compared to the start of training[cite: 249]. Trained agents acquired effective cooperative jamming capabilities[cite: 250].
* **Comparison (MP-DQN vs. Discretized Power)**: Introducing the MP-DQN structure significantly improved performance compared to discretizing the power levels (e.g., using only {0, 0.5, 1})[cite: 251, 255]. Discretization sacrifices precision and prevents selecting optimal power in certain states[cite: 256, 257].
* **Comparison (Double DQN)**: Incorporating Double DQN resulted in faster convergence, lower training variance, and higher average rewards compared to MA-CJD without Double DQN[cite: 258, 259]. It effectively mitigates TD target overestimation[cite: 260, 261].
* **Comparison (vs. Other Methods)**: MA-CJD consistently resulted in fewer radars locking onto defense units compared to QMix (with discretized power), PER-DDQN, rule-based, and random policies. MA-CJD also demonstrated state-dependent adaptive adjustment of jamming power, using minimal power when targets were distant and increasing it as distance decreased.
* **Resource Efficiency**: MA-CJD achieved the shortest average radar lock duration (37.91s) and the lowest total jamming power usage (normalized sum 35.94) compared to other tested strategies, demonstrating significant improvements in both effectiveness and resource conservation. Compared to a rule-based policy, MA-CJD reduced lock duration by 45.98% and power usage by 46.66%.

## 5. Conclusion

The MA-CJD algorithm, implemented using a QMix framework enhanced with MP-DQN for parameterized actions and Double DQN for stability, provides an effective solution for cooperative jamming decision-making in multi-agent scenarios[cite: 291, 294]. Simulation results validate its ability to significantly improve jamming performance (reduced detection time) while minimizing jamming resource consumption compared to baseline methods[cite: 295]. This approach offers a robust framework for multi-dimensional jamming parameter optimization[cite: 296].